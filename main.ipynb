{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f3c2e68d8cd2c13"
  },
  {
   "cell_type": "code",
   "source": [
    "import cudf as pd\n",
    "import dask\n",
    "import numpy as np\n",
    "import pandas\n",
    "import shap\n",
    "import seaborn as sns\n",
    "import numpy\n",
    "import cupy\n",
    "import tensorflow\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from cuml import train_test_split\n",
    "from cuml.svm import svr\n",
    "from cuml import RandomForestRegressor as CudaRandomForest\n",
    "from cuml.metrics import mean_squared_error\n",
    "from dask import multiprocessing\n",
    "from keras import Sequential\n",
    "from keras.src.layers import Input, LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from pyswarms.single import GlobalBestPSO\n",
    "from shap.plots import colors\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "SEED = 100\n",
    "\n",
    "\n",
    "def reset_seed(rnd_seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    random.seed(rnd_seed)\n",
    "    numpy.random.seed(rnd_seed)\n",
    "    cupy.random.seed(rnd_seed)\n",
    "    tensorflow.random.set_seed(rnd_seed)\n",
    "\n",
    "\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "dask.config.set(scheduler=\"threads\", num_workers=4)\n",
    "gc.collect()\n",
    "reset_seed()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T04:12:19.904250Z",
     "start_time": "2024-07-02T04:12:09.003926Z"
    }
   },
   "id": "b6e345ae81073f29",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduardoalba0/.conda/envs/rapids-24.06/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 43885 instead\n",
      "  warnings.warn(\n",
      "2024-07-02 01:12:09,181 - distributed.scheduler - INFO - State start\n",
      "2024-07-02 01:12:09,275 - distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:38639\n",
      "2024-07-02 01:12:09,294 - distributed.scheduler - INFO -   dashboard at:  http://127.0.0.1:43885/status\n",
      "2024-07-02 01:12:09,298 - distributed.scheduler - INFO - Registering Worker plugin shuffle\n",
      "2024-07-02 01:12:09,354 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34445'\n",
      "2024-07-02 01:12:19,398 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39965', name: 0, status: init, memory: 0, processing: 0>\n",
      "2024-07-02 01:12:19,402 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39965\n",
      "2024-07-02 01:12:19,409 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36494\n",
      "2024-07-02 01:12:19,458 - distributed.scheduler - INFO - Receive client connection: Client-46348919-3829-11ef-90d2-00155d2e0be5\n",
      "2024-07-02 01:12:19,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36506\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d53fe60dddad7a9"
  },
  {
   "cell_type": "code",
   "source": [
    "df_electricity = pd.read_csv('./dataset/electricity.csv', sep=\";\", decimal=\".\", header=0)\n",
    "df_climatic = pd.read_csv('./dataset/climatic.csv', sep=\";\", decimal=\".\", header=0)\n",
    "\n",
    "df_electricity[\"data\"] = pd.to_datetime(df_electricity[\"data\"], format=\"%d/%m/%Y\")\n",
    "df_climatic[\"data\"] = pd.to_datetime(df_climatic[\"data\"], format=\"%d/%m/%Y\")\n",
    "\n",
    "df_electricity.set_index(\"data\", inplace=True)\n",
    "df_climatic.set_index(\"data\", inplace=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73429363fb53c4ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pré-Processamento\n",
    "## Dados climáticos faltantes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "559730eb71e02daa"
  },
  {
   "cell_type": "code",
   "source": [
    "for index, row in df_climatic[df_climatic.isnull()].to_pandas().iterrows():\n",
    "    df_mes = df_climatic[df_climatic[\"mes\"] == df_climatic.at[index, \"mes\"]]\n",
    "    for col in row.index:\n",
    "        if pandas.isnull(df_climatic.at[index, col]):\n",
    "            df_mes.at[index, col] = df_mes[col].sum() / df_mes[col][df_mes[col].isnull() == False].count()\n",
    "            df_climatic.at[index, col] = df_mes.at[index, col]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20ae521d263691c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Obtenção dos LAGS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8097a93df49fe1a8"
  },
  {
   "cell_type": "code",
   "source": [
    "for lag_col in [\"consumo\"]:\n",
    "    for i in range(1, 12 + 1):\n",
    "        lag_eletricity = df_electricity[lag_col].shift(i)\n",
    "        df_electricity[f'{lag_col}_LAG_' + '{:02d}'.format(i)] = lag_eletricity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59c87d57d33178",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## União dos dados climáticos aos dados de consumo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf455533d3b84e8b"
  },
  {
   "cell_type": "code",
   "source": [
    "df_electricity = pd.merge(left=df_electricity, right=df_climatic, on=[\"data\", \"mes\", \"ano\"], how=\"left\")\n",
    "df_electricity = df_electricity.drop(\"leitura\", axis=1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "245bb664be419ac8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Criação das variáveis Dummy (mês e ano)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c520eeb0184fc5b"
  },
  {
   "cell_type": "code",
   "source": [
    "df_meses = pd.get_dummies(df_electricity[\"mes\"].astype(int), prefix=\"\", prefix_sep=\"\", dtype=int).rename(\n",
    "    columns={\"1\": \"mes_JAN\", \"2\": \"mes_FEV\", \"3\": \"mes_MAR\", \"4\": \"mes_ABR\", \"5\": \"mes_MAI\", \"6\": \"mes_JUN\",\n",
    "             \"7\": \"mes_JUL\", \"8\": \"mes_AGO\", \"9\": \"mes_SET\", \"10\": \"mes_OUT\", \"11\": \"mes_NOV\", \"12\": \"mes_DEZ\"}\n",
    ")\n",
    "df_anos = pd.get_dummies(df_electricity[\"ano\"].astype(int), prefix=\"\", prefix_sep=\"\", dtype=int).rename(\n",
    "    columns={\"2017\": \"ano_2017\", \"2018\": \"ano_2018\", \"2019\": \"ano_2019\", \"2020\": \"ano_2020\", \"2021\": \"ano_2021\",\n",
    "             \"2022\": \"ano_2022\", \"2023\": \"ano_2023\", \"2024\": \"ano_2024\"}\n",
    ")\n",
    "df_electricity = pd.concat([df_electricity, df_meses, df_anos], axis=1)\n",
    "df_electricity = df_electricity.drop([\"mes\", \"ano\"], axis=1)\n",
    "df_electricity = df_electricity.astype(\"float32\").dropna()\n",
    "\n",
    "df_show = df_electricity.to_pandas()\n",
    "df_show"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3350975c606ba2f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "420d7768ec87dff0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Análise de Correlações\n",
    "## Eletricidade\n",
    "### Correlação com os LAGS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6fbbaaa4d4bf3e0"
  },
  {
   "cell_type": "code",
   "source": [
    "corr_matrix = df_electricity[df_electricity.to_pandas().filter(like=\"consumo\").columns].dropna().to_pandas().corr(\n",
    "    numeric_only=True)\n",
    "sns.heatmap(corr_matrix,\n",
    "            cmap=\"coolwarm\",\n",
    "            center=0,\n",
    "            annot=True,\n",
    "            fmt='.0g')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "203cab94b7b5dc4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlação com as variáveis climáticas"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da6d0ec86da92621"
  },
  {
   "cell_type": "code",
   "source": [
    "corr_matrix = df_electricity.drop(df_electricity.to_pandas().filter(like=\"_LAG_\").columns,\n",
    "                                  axis=1).drop(df_electricity.to_pandas().filter(like=\"mes_\").columns,\n",
    "                                               axis=1).drop(df_electricity.to_pandas().filter(like=\"ano_\").columns,\n",
    "                                                            axis=1).dropna().to_pandas().corr(numeric_only=True)\n",
    "sns.heatmap(corr_matrix,\n",
    "            cmap=\"coolwarm\",\n",
    "            center=0,\n",
    "            annot=True,\n",
    "            fmt='.1g')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a25983797d302706",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Análise dos SHAP values\n",
    "## Eletricidade\n",
    "### Random Forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b63848b390147d73"
  },
  {
   "cell_type": "code",
   "source": [
    "df_electricity_copy = df_electricity.copy().to_pandas()\n",
    "\n",
    "x_electricity = df_electricity_copy.drop(\"consumo\", axis=1)\n",
    "y_electricity = df_electricity_copy[\"consumo\"]\n",
    "model_rf_electr = RandomForestRegressor(n_estimators=100, max_depth=100, random_state=SEED)\n",
    "shap.initjs()\n",
    "\n",
    "model_rf_electr.fit(x_electricity, y_electricity)\n",
    "\n",
    "explainer_rf_electr = shap.Explainer(model_rf_electr)\n",
    "shap_rf_electr = explainer_rf_electr(x_electricity)\n",
    "\n",
    "shap.plots.waterfall(shap_rf_electr[0], max_display=10)\n",
    "shap.plots.force(shap_rf_electr[0])\n",
    "shap.plots.bar(shap_rf_electr)\n",
    "\n",
    "importance_rf_electr = pandas.DataFrame(list(zip(x_electricity.columns, numpy.abs(shap_rf_electr.values).mean(0))),\n",
    "                                        columns=[\"feature\", \"rf importance\"])\n",
    "importance_rf_electr = importance_rf_electr.sort_values(by=[\"rf importance\"])\n",
    "importance_rf_electr"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dc260de37d498c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XGBoost"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "314ed5331c39c10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_electricity_copy = df_electricity.copy().to_pandas()\n",
    "\n",
    "x_electricity = df_electricity_copy.drop(\"consumo\", axis=1)\n",
    "y_electricity = df_electricity_copy[\"consumo\"]\n",
    "\n",
    "model_xgb_electr = XGBRegressor(booster=\"gbtree\", objective='reg:squarederror', random_state=SEED)\n",
    "shap.initjs()\n",
    "\n",
    "model_xgb_electr.fit(x_electricity, y_electricity)\n",
    "\n",
    "explainer_xgb_electr = shap.Explainer(model_xgb_electr)\n",
    "shap_xgb_electr = explainer_xgb_electr(x_electricity)\n",
    "\n",
    "shap.plots.waterfall(shap_xgb_electr[0], max_display=10)\n",
    "shap.plots.force(shap_xgb_electr[0])\n",
    "shap.plots.bar(shap_xgb_electr)\n",
    "\n",
    "importance_xgb_electr = pandas.DataFrame(list(zip(x_electricity.columns, numpy.abs(shap_xgb_electr.values).mean(0))),\n",
    "                                         columns=[\"feature\", \"xgb importance\"])\n",
    "importance_xgb_electr = importance_xgb_electr.sort_values(by=[\"xgb importance\"])\n",
    "importance_xgb_electr"
   ],
   "id": "3dd5d11ee4fdb34f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Média entre RF e XGB",
   "id": "76633c25f52e1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "importance_electr = pandas.DataFrame(list(zip(x_electricity.columns, (\n",
    "        numpy.abs(shap_rf_electr.values).mean(0) + numpy.abs(shap_xgb_electr.values).mean(0)) / 2)),\n",
    "                                     columns=[\"feature\", \"Mean RF/XGB importance\"])\n",
    "importance_electr = importance_electr.sort_values(by=[\"Mean RF/XGB importance\"], ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "bar_features_electr = list(importance_electr[0:9][\"feature\"])\n",
    "bar_features_electr.append(f\"Sum of {len(importance_electr[9:])} other features\")\n",
    "bar_importances_electr = list(importance_electr[0:9][\"Mean RF/XGB importance\"])\n",
    "bar_importances_electr.append(importance_electr[9:][\"Mean RF/XGB importance\"].sum())\n",
    "\n",
    "bar_features_electr = bar_features_electr[::-1]\n",
    "bar_importances_electr = bar_importances_electr[::-1]\n",
    "\n",
    "bars = plt.barh(bar_features_electr, bar_importances_electr, color=colors.red_rgb)\n",
    "\n",
    "for bar, importance in zip(bars, bar_importances_electr):\n",
    "    plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height() / 2, f'+{importance:.2f}',\n",
    "             va='center', ha='left', color=colors.red_rgb)\n",
    "\n",
    "plt.show()\n",
    "importance_electr = importance_electr.sort_values(by=[\"Mean RF/XGB importance\"])\n",
    "importance_electr\n"
   ],
   "id": "ddbe1f0a23c54081",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Seleção de Features\n",
    "## Eletricidade"
   ],
   "id": "931790602e2727cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "importance_electr = importance_electr.sort_values(by=[\"Mean RF/XGB importance\"])\n",
    "\n",
    "\n",
    "def ft_removal_rf(n_ft_removed, dataset):\n",
    "    df_selected = dataset[n_ft_removed:][\"feature\"]\n",
    "\n",
    "    x = df_electricity[df_selected]\n",
    "    y = df_electricity[\"consumo\"]\n",
    "\n",
    "    rf_fs_electr = CudaRandomForest(n_bins=1, random_state=SEED)\n",
    "\n",
    "    cvs_electricity = []\n",
    "    for i_train, i_test in TimeSeriesSplit(n_splits=6, test_size=1).split(x, y):\n",
    "        x_train, x_test = x.iloc[i_train].to_cupy(), x.iloc[i_test].to_cupy()\n",
    "        y_train, y_test = y.iloc[i_train].to_cupy(), y.iloc[i_test].to_cupy()\n",
    "\n",
    "        rf_fs_electr.fit(x_train, y_train)\n",
    "        cvs_electricity.append(int(mean_squared_error(y_test, rf_fs_electr.predict(x_test)).get()))\n",
    "\n",
    "    return int(numpy.array(cvs_electricity).mean())\n",
    "\n",
    "\n",
    "def ft_removal_xgb(n_ft_removed, dataset):\n",
    "    df_selected = dataset[n_ft_removed:][\"feature\"]\n",
    "\n",
    "    x = df_electricity[df_selected]\n",
    "    y = df_electricity[\"consumo\"]\n",
    "\n",
    "    xgb_fs_electr = XGBRegressor(booster=\"gbtree\", device=\"cuda\", random_state=SEED)\n",
    "\n",
    "    cvs_electricity = []\n",
    "    for i_train, i_test in TimeSeriesSplit(n_splits=6, test_size=1).split(x, y):\n",
    "        x_train, x_test = x.iloc[i_train].to_cupy(), x.iloc[i_test].to_cupy()\n",
    "        y_train, y_test = y.iloc[i_train].to_cupy(), y.iloc[i_test].to_cupy()\n",
    "\n",
    "        xgb_fs_electr.fit(x_train, y_train)\n",
    "        cvs_electricity.append(int(mean_squared_error(y_test, xgb_fs_electr.predict(x_test)).get()))\n",
    "\n",
    "    return int(numpy.array(cvs_electricity).mean())\n",
    "\n",
    "\n",
    "importance_electr = importance_electr.sort_values(by=[\"Mean RF/XGB importance\"])\n",
    "\n",
    "ft_rm_rf_electr = dask.compute(\n",
    "    [dask.delayed(ft_removal_rf)(i, importance_electr) for i in range(importance_electr[\"feature\"].shape[0])])[0]\n",
    "ft_rm_xgb_electr = dask.compute(\n",
    "    [dask.delayed(ft_removal_xgb)(i, importance_electr) for i in range(importance_electr[\"feature\"].shape[0])])[0]\n",
    "\n"
   ],
   "id": "99342018e98e7446",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ft_rm_mean_electr = (np.array(ft_rm_rf_electr) + np.array(ft_rm_xgb_electr)) / 2\n",
    "\n",
    "plt.plot([-x for x in ft_rm_rf_electr[:40]], label=\"RF\")\n",
    "plt.plot([-x for x in ft_rm_xgb_electr[:40]], label=\"XGBoost\")\n",
    "plt.plot([-x for x in ft_rm_mean_electr[:40]], label=\"XGBoost/RF Mean\")\n",
    "plt.xlabel('Number of features removed')\n",
    "plt.ylabel('Neg. Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "importance_electr = importance_electr.sort_values(by=[\"Mean RF/XGB importance\"])\n",
    "min_index = np.argmin(ft_rm_mean_electr[:40])\n",
    "df_electricity_selected = df_electricity.drop(importance_electr[:min_index][\"feature\"], axis=1)\n",
    "df_electricity_selected.to_pandas().to_csv(f\"dataset/elect_merged_selected.csv\", sep=\";\", decimal=\",\")\n",
    "df_electricity_selected"
   ],
   "id": "83941ef17652fca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T04:12:28.657272Z",
     "start_time": "2024-07-02T04:12:24.232292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_electricity_selected = pd.read_csv('./dataset/elect_merged_selected.csv', sep=\";\", decimal=\",\", header=0).set_index(\n",
    "    \"data\")\n",
    "df_electricity_selected = df_electricity_selected.astype(\"float32\")"
   ],
   "id": "6701e71637a0911f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cudf.core.dataframe.DataFrame'>\n",
      "Index: 67 entries, 2019-01-01 to 2024-03-01\n",
      "Data columns (total 17 columns):\n",
      " #   Column               Non-Null Count  Dtype\n",
      "---  ------               --------------  -----\n",
      " 0   consumo              67 non-null     float32\n",
      " 1   consumo_LAG_01       67 non-null     float32\n",
      " 2   consumo_LAG_02       67 non-null     float32\n",
      " 3   consumo_LAG_03       67 non-null     float32\n",
      " 4   consumo_LAG_04       67 non-null     float32\n",
      " 5   consumo_LAG_06       67 non-null     float32\n",
      " 6   consumo_LAG_09       67 non-null     float32\n",
      " 7   consumo_LAG_12       67 non-null     float32\n",
      " 8   tempmed              67 non-null     float32\n",
      " 9   tempmax_abs          67 non-null     float32\n",
      " 10  tempmax_med_abs      67 non-null     float32\n",
      " 11  tempmin_abs          67 non-null     float32\n",
      " 12  velventomax_abs      67 non-null     float32\n",
      " 13  velventomax_med_abs  67 non-null     float32\n",
      " 14  mes_JAN              67 non-null     float32\n",
      " 15  mes_MAR              67 non-null     float32\n",
      " 16  ano_2019             67 non-null     float32\n",
      "dtypes: float32(17)\n",
      "memory usage: 5.4+ KB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configuração dos Otimizadores\n",
    "## Algoritmo Genético\n",
    "### Random Forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a20cb439079a737"
  },
  {
   "cell_type": "code",
   "source": [
    "class IndRF:\n",
    "    def __init__(self):\n",
    "        self.fitness = None\n",
    "        self.seed = None\n",
    "        self.estimators = 0\n",
    "        self.max_depth = 0\n",
    "        self.min_samples_split = 0\n",
    "        self.min_samples_leaf = 0\n",
    "\n",
    "    def create_random(self):\n",
    "        self.rand_estimators()\n",
    "        self.rand_depth()\n",
    "        self.rand_samples_split()\n",
    "        self.rand_samples_leaf()\n",
    "        return self\n",
    "\n",
    "    def rand_estimators(self):\n",
    "        self.estimators = random.randint(1, 300)\n",
    "\n",
    "    def rand_depth(self):\n",
    "        self.max_depth = random.randint(1, 300)\n",
    "\n",
    "    def rand_samples_split(self):\n",
    "        self.min_samples_split = random.randint(2, 50)\n",
    "\n",
    "    def rand_samples_leaf(self):\n",
    "        self.min_samples_leaf = random.randint(1, 50)\n",
    "\n",
    "\n",
    "class GARF:\n",
    "    def __init__(self, dataset, n_individuals, n_generations, mutation_rate, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.n_individuals = n_individuals\n",
    "        self.n_generations = n_generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.population = {}\n",
    "        self.iters = []\n",
    "        self.init_pop()\n",
    "        self.init_gen()\n",
    "\n",
    "    def init_pop(self):\n",
    "        futures = [dask.delayed(self.create_ind)(_) for _ in range(self.n_individuals)]\n",
    "        self.population = dask.compute(futures)[0]\n",
    "        self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "        self.iters.append(self.population[0])\n",
    "\n",
    "    def create_ind(self, i):\n",
    "        print(f'Ind:{i}')\n",
    "        ind = IndRF().create_random()\n",
    "        ind.seed = self.seed\n",
    "        ind = self.get_fitness(ind)\n",
    "        return ind\n",
    "\n",
    "    def init_gen(self):\n",
    "        for i in range(self.n_generations):\n",
    "            print(f\"Iter: {i}\")\n",
    "            new_seed = self.seed * (2 + i)\n",
    "            reset_seed(new_seed)\n",
    "            ind_a = random.choice(self.population)\n",
    "            ind_b = random.choice(self.population)\n",
    "            ind_c = self.crossover(ind_a, ind_b)\n",
    "            ind_c.seed = new_seed\n",
    "\n",
    "            if random.uniform(0, 1) < self.mutation_rate:\n",
    "                ind_c = self.mutation(ind_c)\n",
    "\n",
    "            ind_c = self.get_fitness(ind_c)\n",
    "            self.population.append(ind_c)\n",
    "            self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "            self.population = self.population[:self.n_individuals - 1]\n",
    "            self.iters.append(self.population[0])\n",
    "            \n",
    "            self.save_pop_csv()\n",
    "            self.save_iters_csv()\n",
    "            \n",
    "            del new_seed\n",
    "            gc.collect()\n",
    "\n",
    "    def mutation(self, ind):\n",
    "        random.choice([\n",
    "            ind.rand_estimators(),\n",
    "            ind.rand_depth(),\n",
    "            ind.rand_samples_split(),\n",
    "            ind.rand_samples_leaf(),\n",
    "        ])\n",
    "        return ind\n",
    "\n",
    "    def crossover(self, ind_a, ind_b):\n",
    "        ind = IndRF()\n",
    "        ind.estimators = random.choice([ind_a.estimators, ind_b.estimators])\n",
    "        ind.max_depth = random.choice([ind_a.max_depth, ind_b.max_depth])\n",
    "        ind.min_samples_split = random.choice([ind_a.min_samples_split, ind_b.min_samples_split])\n",
    "        ind.min_samples_leaf = random.choice([ind_a.min_samples_leaf, ind_b.min_samples_leaf])\n",
    "        return ind\n",
    "\n",
    "    def get_fitness(self, individual):\n",
    "        search = list(filter(lambda ind:\n",
    "                             ind.estimators == individual.estimators and\n",
    "                             ind.max_depth == individual.max_depth and\n",
    "                             ind.min_samples_split == individual.min_samples_split and\n",
    "                             ind.min_samples_leaf == individual.min_samples_leaf, self.population))\n",
    "\n",
    "        if search:\n",
    "            return search[0]\n",
    "\n",
    "        x = self.dataset.drop(\"consumo\", axis=1)\n",
    "        y = self.dataset[\"consumo\"]\n",
    "\n",
    "        model = CudaRandomForest(random_state=self.seed,\n",
    "                                 n_estimators=individual.estimators,\n",
    "                                 max_depth=individual.max_depth,\n",
    "                                 min_samples_split=individual.min_samples_split,\n",
    "                                 min_samples_leaf=individual.min_samples_leaf,\n",
    "                                 n_streams=individual.estimators,\n",
    "                                 n_bins=x.shape[0])\n",
    "\n",
    "        csv = []\n",
    "        for i_train, i_test in TimeSeriesSplit(n_splits=6, test_size=1).split(x, y):\n",
    "            x_train, x_test = x.iloc[i_train].to_cupy().get(), x.iloc[i_test].to_cupy().get()\n",
    "            y_train, y_test = y.iloc[i_train].to_cupy().get(), y.iloc[i_test].to_cupy().get()\n",
    "\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            csv.append(int(mean_squared_error(y_test, model.predict(x_test).get())))\n",
    "            del x_train, x_test, y_train, y_test\n",
    "\n",
    "        individual.fitness = int(numpy.array(csv).mean())\n",
    "\n",
    "        del x, y, csv, i_train, i_test, model\n",
    "        gc.collect()\n",
    "        return individual\n",
    "\n",
    "    def population_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for ind in self.population:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"N_estimators\": ind.estimators,\n",
    "                \"Max_depth\": ind.max_depth,\n",
    "                \"Min_samples_split\": ind.min_samples_split,\n",
    "                \"Min_samples_leaf\": ind.min_samples_leaf,\n",
    "                \"Seed\": ind.seed,\n",
    "                \"Fitness\": ind.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for ind in self.iters:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"N_estimators\": ind.estimators,\n",
    "                \"Max_depth\": ind.max_depth,\n",
    "                \"Min_samples_split\": ind.min_samples_split,\n",
    "                \"Min_samples_leaf\": ind.min_samples_leaf,\n",
    "                \"Seed\": ind.seed,\n",
    "                \"Fitness\": ind.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def save_pop_csv(self):\n",
    "        pd_df = self.population_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/GA-RF POP SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/GA-RF ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T04:13:10.380835Z",
     "start_time": "2024-07-02T04:13:10.322077Z"
    }
   },
   "id": "66c830f88912d062",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "### XGBoost",
   "metadata": {
    "collapsed": false
   },
   "id": "3a77d8f401e9367b"
  },
  {
   "cell_type": "code",
   "source": [
    "class IndXGB:\n",
    "    def __init__(self):\n",
    "        self.fitness = None\n",
    "        self.seed = None\n",
    "        self.estimators = 0\n",
    "        self.max_depth = 0\n",
    "        self.booster = None\n",
    "        self.reg_lambda = 0\n",
    "        self.reg_alpha = 0\n",
    "\n",
    "    def create_random(self):\n",
    "        self.rand_estimators()\n",
    "        self.rand_depth()\n",
    "        self.rand_booster()\n",
    "        self.rand_lambda()\n",
    "        self.rand_alpha()\n",
    "        return self\n",
    "\n",
    "    def rand_estimators(self):\n",
    "        self.estimators = random.randint(1, 300)\n",
    "\n",
    "    def rand_depth(self):\n",
    "        self.max_depth = random.randint(1, 300)\n",
    "\n",
    "    def rand_booster(self):\n",
    "        self.booster = random.choice([\"gbtree\", \"gblinear\", \"dart\"])\n",
    "\n",
    "    def rand_lambda(self):\n",
    "        self.reg_lambda = random.uniform(0, 100)\n",
    "\n",
    "    def rand_alpha(self):\n",
    "        self.reg_alpha = random.uniform(0, 100)\n",
    "\n",
    "\n",
    "class GAXGB:\n",
    "    def __init__(self, dataset, n_individuals, n_generations, mutation_rate, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.n_individuals = n_individuals\n",
    "        self.n_generations = n_generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.population = []\n",
    "        self.iters = []\n",
    "        self.init_pop()\n",
    "        self.init_gen()\n",
    "\n",
    "    def init_pop(self):\n",
    "        futures = [dask.delayed(self.create_ind)(_) for _ in range(self.n_individuals)]\n",
    "        self.population = dask.compute(futures)[0]\n",
    "        self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "        self.iters.append(self.population[0])\n",
    "\n",
    "    def create_ind(self, i):\n",
    "        print(f'Ind:{i}')\n",
    "        ind = IndXGB().create_random()\n",
    "        ind = self.get_fitness(ind)\n",
    "        self.population.append(ind)\n",
    "\n",
    "    def init_gen(self):\n",
    "        for i in range(self.n_generations):\n",
    "            print(f\"Iter: {i}\")\n",
    "            new_seed = self.seed * (2 + i)\n",
    "            reset_seed(new_seed)\n",
    "            ind_a = random.choice(self.population)\n",
    "            ind_b = random.choice(self.population)\n",
    "            ind_c = self.crossover(ind_a, ind_b)\n",
    "            ind_c.seed = new_seed\n",
    "\n",
    "            if random.uniform(0, 1) < self.mutation_rate:\n",
    "                ind_c = self.mutation(ind_c)\n",
    "\n",
    "            ind_c = self.get_fitness(ind_c)\n",
    "            self.population.append(ind_c)\n",
    "            self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "            self.population = self.population[:self.n_individuals - 1]\n",
    "            self.iters.append(self.population[0])\n",
    "\n",
    "            self.save_pop_csv()\n",
    "            self.save_iters_csv()\n",
    "            \n",
    "            del new_seed\n",
    "            gc.collect()\n",
    "\n",
    "    def mutation(self, ind):\n",
    "        random.choice([\n",
    "            ind.rand_estimators(),\n",
    "            ind.rand_depth(),\n",
    "            ind.rand_booster(),\n",
    "            ind.rand_lambda(),\n",
    "            ind.rand_alpha()\n",
    "        ])\n",
    "        return ind\n",
    "\n",
    "    def crossover(self, ind_a, ind_b):\n",
    "        ind = IndXGB()\n",
    "        ind.estimators = random.choice([ind_a.estimators, ind_b.estimators])\n",
    "        ind.max_depth = random.choice([ind_a.max_depth, ind_b.max_depth])\n",
    "        ind.booster = random.choice([ind_a.booster, ind_b.booster])\n",
    "        ind.reg_lambda = random.choice([ind_a.reg_lambda, ind_b.reg_lambda])\n",
    "        ind.reg_alpha = random.choice([ind_a.reg_alpha, ind_b.reg_alpha])\n",
    "        return ind\n",
    "\n",
    "    def get_fitness(self, individual):\n",
    "        search = list(filter(lambda ind:\n",
    "                             ind.estimators == individual.estimators and\n",
    "                             ind.max_depth == individual.max_depth and\n",
    "                             ind.booster == individual.booster and\n",
    "                             ind.reg_lambda == individual.reg_lambda and\n",
    "                             ind.reg_alpha == individual.reg_alpha, self.population))\n",
    "\n",
    "        if search:\n",
    "            return search[0]\n",
    "\n",
    "        x = self.dataset.drop(\"consumo\", axis=1)\n",
    "        y = self.dataset[\"consumo\"]\n",
    "\n",
    "        updater = \"coord_descent\" if individual.booster == \"gblinear\" else None\n",
    "\n",
    "        model = XGBRegressor(device=\"cuda\", random_state=self.seed,\n",
    "                             n_estimators=individual.estimators,\n",
    "                             max_depth=individual.max_depth,\n",
    "                             booster=individual.booster,\n",
    "                             reg_lambda=individual.reg_lambda,\n",
    "                             reg_alpha=individual.reg_alpha,\n",
    "                             updater=updater)\n",
    "\n",
    "        csv = []\n",
    "        for i_train, i_test in TimeSeriesSplit(n_splits=6, test_size=1).split(x, y):\n",
    "            x_train, x_test = x.iloc[i_train].to_cupy().get(), x.iloc[i_test].to_cupy().get()\n",
    "            y_train, y_test = y.iloc[i_train].to_cupy().get(), y.iloc[i_test].to_cupy().get()\n",
    "\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            csv.append(int(mean_squared_error(y_test, model.predict(x_test).get())))\n",
    "            del x_train, x_test, y_train, y_test\n",
    "\n",
    "        individual.fitness = int(numpy.array(csv).mean())\n",
    "\n",
    "        del x, y, csv, i_train, i_test, model\n",
    "        gc.collect()\n",
    "        return individual\n",
    "\n",
    "    def population_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for ind in self.population:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"N_estimators\": ind.estimators,\n",
    "                \"Max_depth\": ind.max_depth,\n",
    "                \"Booster\": ind.booster,\n",
    "                \"Lambda\": ind.reg_lambda,\n",
    "                \"Alpha\": ind.reg_alpha,\n",
    "                \"Seed\": ind.seed,\n",
    "                \"Fitness\": ind.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for ind in self.iters:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"N_estimators\": ind.estimators,\n",
    "                \"Max_depth\": ind.max_depth,\n",
    "                \"Booster\": ind.booster,\n",
    "                \"Lambda\": ind.reg_lambda,\n",
    "                \"Alpha\": ind.reg_alpha,\n",
    "                \"Seed\": ind.seed,\n",
    "                \"Fitness\": ind.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def save_pop_csv(self):\n",
    "        pd_df = self.population_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/GA-XGB POP SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/GA-XGB ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T04:13:12.493366Z",
     "start_time": "2024-07-02T04:13:12.434447Z"
    }
   },
   "id": "75eecff7358788e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SVR",
   "id": "9e39a7cabe57b126"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T04:13:14.018350Z",
     "start_time": "2024-07-02T04:13:13.948873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IndSVR:\n",
    "    def __init__(self):\n",
    "        self.fitness = None\n",
    "        self.seed = None\n",
    "        self.c = 0\n",
    "        self.epsilon = 0\n",
    "        self.degree = 0\n",
    "        self.kernel = None\n",
    "        self.gamma = None\n",
    "\n",
    "    def create_random(self):\n",
    "        self.rand_c()\n",
    "        self.rand_epsilon()\n",
    "        self.rand_kernel()\n",
    "        return self\n",
    "\n",
    "    def rand_c(self):\n",
    "        self.c = random.uniform(0.001, 300)\n",
    "\n",
    "    def rand_epsilon(self):\n",
    "        self.epsilon = random.uniform(0.001, 300)\n",
    "\n",
    "    def rand_kernel(self):\n",
    "        self.kernel = random.choice([\"poly\", \"rbf\", \"sigmoid\"])\n",
    "\n",
    "\n",
    "class GASVR:\n",
    "    def __init__(self, dataset, n_individuals, n_generations, mutation_rate, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.n_individuals = n_individuals\n",
    "        self.n_generations = n_generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.population = []\n",
    "        self.iters = []\n",
    "        self.init_pop()\n",
    "        self.init_gen()\n",
    "\n",
    "    def init_pop(self):\n",
    "        futures = [dask.delayed(self.create_ind)(_) for _ in range(self.n_individuals)]\n",
    "        self.population = dask.compute(futures)[0]\n",
    "        self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "        self.iters.append(self.population[0])\n",
    "\n",
    "    def create_ind(self, i):\n",
    "        ind = IndSVR().create_random()\n",
    "        ind = self.get_fitness(ind)\n",
    "        self.population.append(ind)\n",
    "        print(f'Ind:{i}')\n",
    "        return ind\n",
    "\n",
    "    def init_gen(self):\n",
    "        for i in range(self.n_generations):\n",
    "            print(f\"Iter: {i}\")\n",
    "            new_seed = self.seed * (2 + i)\n",
    "            reset_seed(new_seed)\n",
    "            ind_a = random.choice(self.population)\n",
    "            ind_b = random.choice(self.population)\n",
    "            ind_c = self.crossover(ind_a, ind_b)\n",
    "            ind_c.seed = new_seed\n",
    "\n",
    "            if random.uniform(0, 1) < self.mutation_rate:\n",
    "                ind_c = self.mutation(ind_c)\n",
    "\n",
    "            ind_c = self.get_fitness(ind_c)\n",
    "            self.population.append(ind_c)\n",
    "            self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "            self.population = self.population[:self.n_individuals - 1]\n",
    "            self.iters.append(self.population[0])\n",
    "\n",
    "            self.save_pop_csv()\n",
    "            self.save_iters_csv()\n",
    "\n",
    "            del new_seed\n",
    "            gc.collect()\n",
    "\n",
    "    def mutation(self, ind):\n",
    "        random.choice([\n",
    "            ind.rand_c(),\n",
    "            ind.rand_epsilon(),\n",
    "            ind.rand_kernel(),\n",
    "        ])\n",
    "        return ind\n",
    "\n",
    "    def crossover(self, ind_a, ind_b):\n",
    "        ind = IndSVR()\n",
    "        ind.c = random.choice([ind_a.c, ind_b.c])\n",
    "        ind.epsilon = random.choice([ind_a.epsilon, ind_b.epsilon])\n",
    "        ind.kernel = random.choice([ind_a.kernel, ind_b.kernel])\n",
    "        return ind\n",
    "\n",
    "    def get_fitness(self, individual):\n",
    "        search = list(filter(lambda ind:\n",
    "                             ind.c == individual.c and\n",
    "                             ind.epsilon == individual.epsilon and\n",
    "                             ind.degree == individual.degree and\n",
    "                             ind.kernel == individual.kernel and\n",
    "                             ind.gamma == individual.gamma, self.population))\n",
    "\n",
    "        if search:\n",
    "            return search[0]\n",
    "\n",
    "        x = self.dataset.drop(\"consumo\", axis=1)\n",
    "        y = self.dataset[\"consumo\"]\n",
    "\n",
    "        model = svr.SVR(C=individual.c,\n",
    "                        epsilon=individual.epsilon,\n",
    "                        kernel=individual.kernel)\n",
    "\n",
    "        csv = []\n",
    "        for i_train, i_test in TimeSeriesSplit(n_splits=6, test_size=1).split(x, y):\n",
    "            x_train, x_test = x.iloc[i_train].to_cupy().get(), x.iloc[i_test].to_cupy().get()\n",
    "            y_train, y_test = y.iloc[i_train].to_cupy().get(), y.iloc[i_test].to_cupy().get()\n",
    "\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            csv.append(int(mean_squared_error(y_test, model.predict(x_test).get())))\n",
    "            del x_train, x_test, y_train, y_test\n",
    "\n",
    "        individual.fitness = int(numpy.array(csv).mean())\n",
    "\n",
    "        del x, y, csv, i_train, i_test, model\n",
    "        gc.collect()\n",
    "        return individual\n",
    "\n",
    "    def population_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for ind in self.population:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"C\": ind.c,\n",
    "                \"Epsilon\": ind.epsilon,\n",
    "                \"Kernel\": ind.kernel,\n",
    "                \"Seed\": ind.seed,\n",
    "                \"Fitness\": ind.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for ind in self.iters:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"C\": ind.c,\n",
    "                \"Epsilon\": ind.epsilon,\n",
    "                \"Kernel\": ind.kernel,\n",
    "                \"Seed\": ind.seed,\n",
    "                \"Fitness\": ind.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def save_pop_csv(self):\n",
    "        pd_df = self.population_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/GA-SVR POP SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/GA-SVR ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n"
   ],
   "id": "7ca57f474cad4d70",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LSTM",
   "id": "2e046bb032c757a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T04:13:15.060457Z",
     "start_time": "2024-07-02T04:13:14.929901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IndLSTM:\n",
    "    def __init__(self):\n",
    "        self.fitness = None\n",
    "        self.seed = None\n",
    "        self.lstm_units = 0\n",
    "        self.epochs = 0\n",
    "        self.batch_size = 0\n",
    "        self.lstm_activation = None\n",
    "        self.bias = None\n",
    "\n",
    "    def create_random(self):\n",
    "        self.rand_units()\n",
    "        self.rand_epochs()\n",
    "        self.rand_batch()\n",
    "        self.rand_activation()\n",
    "        self.rand_bias()\n",
    "        return self\n",
    "\n",
    "    def rand_units(self):\n",
    "        self.lstm_units = random.randint(1, 300)\n",
    "\n",
    "    def rand_epochs(self):\n",
    "        self.epochs = random.randint(1, 100)\n",
    "\n",
    "    def rand_batch(self):\n",
    "        self.batch_size = random.randint(1, 300)\n",
    "\n",
    "    def rand_activation(self):\n",
    "        self.lstm_activation = random.choice(\n",
    "            [\"linear\", \"mish\", \"sigmoid\", \"softmax\", \"softplus\", \"softsign\", \"tanh\", None])\n",
    "\n",
    "    def rand_bias(self):\n",
    "        self.bias = random.choice([False, True])\n",
    "\n",
    "\n",
    "class GALSTM:\n",
    "    def __init__(self, dataset, n_individuals, n_generations, mutation_rate, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.n_individuals = n_individuals\n",
    "        self.n_generations = n_generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.population = []\n",
    "        self.iters = []\n",
    "        self.init_pop()\n",
    "        self.init_gen()\n",
    "\n",
    "    def init_pop(self):\n",
    "        futures = [dask.delayed(self.create_ind)(_) for _ in range(self.n_individuals)]\n",
    "        self.population = dask.compute(futures)[0]\n",
    "        self.population = sorted(self.population, key=lambda a: a.fitness)\n",
    "        self.iters.append(self.population[0])\n",
    "\n",
    "    def create_ind(self, i):\n",
    "        print(f'Ind:{i}')\n",
    "        ind = IndLSTM().create_random()\n",
    "        ind = self.get_fitness(ind)\n",
    "        self.population.append(ind)\n",
    "        return ind\n",
    "\n",
    "    def init_gen(self):\n",
    "        for i in range(self.n_generations):\n",
    "            print(f\"Iter: {i}\")\n",
    "            new_seed = self.seed * (2 + i)\n",
    "            reset_seed(new_seed)\n",
    "            ind_a = random.choice(self.population)\n",
    "            ind_b = random.choice(self.population)\n",
    "            ind_c = self.crossover(ind_a, ind_b)\n",
    "            ind_c.seed = new_seed\n",
    "\n",
    "            if random.uniform(0, 1) < self.mutation_rate:\n",
    "                ind_c = self.mutation(ind_c)\n",
    "\n",
    "            ind_c = self.get_fitness(ind_c)\n",
    "            self.population.append(ind_c)\n",
    "            self.population = list(sorted(self.population, key=lambda a: a.fitness))\n",
    "            self.population = self.population[:self.n_individuals - 1]\n",
    "            self.iters.append(self.population[0])\n",
    "\n",
    "            self.save_pop_csv()\n",
    "            self.save_iters_csv()\n",
    "\n",
    "            del new_seed\n",
    "            gc.collect()\n",
    "\n",
    "    def mutation(self, ind):\n",
    "        random.choice([\n",
    "            ind.rand_units(),\n",
    "            ind.rand_epochs(),\n",
    "            ind.rand_batch(),\n",
    "            ind.rand_activation(),\n",
    "            ind.rand_bias()\n",
    "        ])\n",
    "        return ind\n",
    "\n",
    "    def crossover(self, ind_a, ind_b):\n",
    "        ind = IndLSTM()\n",
    "        ind.lstm_units = random.choice([ind_a.lstm_units, ind_b.lstm_units])\n",
    "        ind.epochs = random.choice([ind_a.epochs, ind_b.epochs])\n",
    "        ind.batch_size = random.choice([ind_a.batch_size, ind_b.batch_size])\n",
    "        ind.lstm_activation = random.choice([ind_a.lstm_activation, ind_b.lstm_activation])\n",
    "        ind.bias = random.choice([ind_a.bias, ind_b.bias])\n",
    "        return ind\n",
    "\n",
    "    def get_fitness(self, individual):\n",
    "        print(f\"Units: {individual.lstm_units}\" +\n",
    "              f\"Epochs: {individual.epochs}\" +\n",
    "              f\"Batch Size: {individual.batch_size}\" +\n",
    "              f\"Activation: {individual.lstm_activation}\" +\n",
    "              f\"Bias: {individual.bias}\")\n",
    "\n",
    "        search = list(filter(lambda ind:\n",
    "                             ind.lstm_units == individual.lstm_units and\n",
    "                             ind.epochs == individual.epochs and\n",
    "                             ind.batch_size == individual.batch_size and\n",
    "                             ind.lstm_activation == individual.lstm_activation and\n",
    "                             ind.bias == individual.bias, self.population))\n",
    "\n",
    "        if search:\n",
    "            return search[0]\n",
    "\n",
    "        x = self.dataset.drop(\"consumo\", axis=1)\n",
    "        y = self.dataset[\"consumo\"]\n",
    "\n",
    "        model = Sequential([\n",
    "            Input((x.shape[1], 1)),\n",
    "            LSTM(individual.lstm_units,\n",
    "                 activation=individual.lstm_activation,\n",
    "                 use_bias=individual.bias,\n",
    "                 seed=self.seed),\n",
    "            Dense(1),\n",
    "        ])\n",
    "        model.compile(loss='mse')\n",
    "\n",
    "        csv = []\n",
    "        for i_train, i_test in TimeSeriesSplit(n_splits=6, test_size=1).split(x, y):\n",
    "            x_train, x_test = x.iloc[i_train].to_cupy().get(), x.iloc[i_test].to_cupy().get()\n",
    "            y_train, y_test = y.iloc[i_train].to_cupy().get(), y.iloc[i_test].to_cupy().get()\n",
    "\n",
    "            model.fit(x_train, y_train, shuffle=False, verbose=False, epochs=individual.epochs,\n",
    "                      batch_size=individual.batch_size)\n",
    "            csv.append(int(mean_squared_error(y_test, model.predict(x_test)[0])))\n",
    "            del x_train, x_test, y_train, y_test\n",
    "\n",
    "        individual.fitness = int(numpy.array(csv).mean())\n",
    "\n",
    "        del x, y, csv, i_train, i_test, model\n",
    "        gc.collect()\n",
    "        return individual\n",
    "\n",
    "    def population_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for ind in self.population:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"Units\": ind.lstm_units,\n",
    "                \"Epochs\": ind.epochs,\n",
    "                \"Batch Size\": ind.batch_size,\n",
    "                \"Activation\": ind.lstm_activation,\n",
    "                \"Bias\": ind.bias,\n",
    "                \"Seed\": ind.seed,\n",
    "                \"Fitness\": ind.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for ind in self.iters:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"Units\": ind.lstm_units,\n",
    "                \"Epochs\": ind.epochs,\n",
    "                \"Batch Size\": ind.batch_size,\n",
    "                \"Activation\": ind.lstm_activation,\n",
    "                \"Bias\": ind.bias,\n",
    "                \"Seed\": ind.seed,\n",
    "                \"Fitness\": ind.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def save_pop_csv(self):\n",
    "        pd_df = self.population_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/GA-LSTM POP SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/GA-LSTM ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n"
   ],
   "id": "3f1144f41d636747",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Enxame de Partículas\n",
    "### Random Forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "921a2196f8f7a5a6"
  },
  {
   "cell_type": "code",
   "source": [
    "class PartRF:\n",
    "    def __init_(self):\n",
    "        self.fitness = None\n",
    "        self.seed = None\n",
    "        self.estimators = 0\n",
    "        self.max_depth = 0\n",
    "        self.min_samples_split = 0\n",
    "        self.min_samples_leaf = 0\n",
    "\n",
    "\n",
    "class PSORF:\n",
    "    def __init__(self, dataset, n_particles, n_iters, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iters = n_iters\n",
    "        self.particles = []\n",
    "        self.iters = []\n",
    "        self.run()\n",
    "\n",
    "    def run(self):\n",
    "        lower_bound = [1, 1, 2, 1]\n",
    "        uppper_bound = [300, 300, 50, 50]\n",
    "        bounds = (lower_bound, uppper_bound)\n",
    "\n",
    "        options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
    "        optimizer = GlobalBestPSO(n_particles=self.n_particles,\n",
    "                                  dimensions=4,\n",
    "                                  options=options,\n",
    "                                  bounds=bounds)\n",
    "\n",
    "        optimizer.optimize(self.get_fitness, iters=self.n_iters)\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "\n",
    "    def get_fitness(self, parts):\n",
    "        parts = np.round(parts)\n",
    "        fit_lst = dask.compute([dask.delayed(self.objective_function)(parts[j]) for j in range(self.n_particles)])[0]\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "        self.iters.append(self.particles[0])\n",
    "        \n",
    "        self.save_parts_csv()\n",
    "        self.save_iters_csv()\n",
    "\n",
    "        gc.collect()\n",
    "        return fit_lst\n",
    "\n",
    "    def objective_function(self, particle_arr):\n",
    "        new_seed = self.seed * (2 + len(self.particles) / self.n_particles)\n",
    "        reset_seed(new_seed)\n",
    "        particle = PartRF()\n",
    "        particle.estimators = int(particle_arr[0])\n",
    "        particle.max_depth = int(particle_arr[1])\n",
    "        particle.min_samples_split = int(particle_arr[2])\n",
    "        particle.min_samples_leaf = int(particle_arr[3])\n",
    "        particle.seed = new_seed\n",
    "\n",
    "        search = list(filter(lambda par:\n",
    "                             par.estimators == particle.estimators and\n",
    "                             par.max_depth == particle.max_depth and\n",
    "                             par.min_samples_split == particle.min_samples_split and\n",
    "                             par.min_samples_leaf == particle.min_samples_leaf, self.particles))\n",
    "\n",
    "        if search:\n",
    "            self.particles.append(search[0])\n",
    "            return search[0].fitness\n",
    "\n",
    "        x = self.dataset.drop(\"consumo\", axis=1)\n",
    "        y = self.dataset[\"consumo\"]\n",
    "\n",
    "        model = CudaRandomForest(random_state=self.seed,\n",
    "                                 n_estimators=particle.estimators,\n",
    "                                 max_depth=particle.max_depth,\n",
    "                                 min_samples_split=particle.min_samples_split,\n",
    "                                 min_samples_leaf=particle.min_samples_leaf,\n",
    "                                 n_streams=particle.estimators,\n",
    "                                 n_bins=x.shape[0])\n",
    "\n",
    "        csv = []\n",
    "        for i_train, i_test in TimeSeriesSplit(n_splits=6, test_size=1).split(x, y):\n",
    "            x_train, x_test = x.iloc[i_train].to_cupy().get(), x.iloc[i_test].to_cupy().get()\n",
    "            y_train, y_test = y.iloc[i_train].to_cupy().get(), y.iloc[i_test].to_cupy().get()\n",
    "\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            csv.append(int(mean_squared_error(y_test, model.predict(x_test).get())))\n",
    "            del x_train, x_test, y_train, y_test\n",
    "\n",
    "        self.particles.append(particle)\n",
    "\n",
    "        del new_seed, x, y, csv, i_train, i_test, model\n",
    "        gc.collect()\n",
    "        return particle.fitness\n",
    "\n",
    "    def particles_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for part in self.particles:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"N_estimators\": part.estimators,\n",
    "                \"Max_depth\": part.max_depth,\n",
    "                \"Min_samples_split\": part.min_samples_split,\n",
    "                \"Min_samples_leaf\": part.min_samples_leaf,\n",
    "                \"Seed\": part.seed,\n",
    "                \"Fitness\": part.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for part in self.iters:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"N_estimators\": part.estimators,\n",
    "                \"Max_depth\": part.max_depth,\n",
    "                \"Min_samples_split\": part.min_samples_split,\n",
    "                \"Min_samples_leaf\": part.min_samples_leaf,\n",
    "                \"Seed\": part.seed,\n",
    "                \"Fitness\": part.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def save_parts_csv(self):\n",
    "        pd_df = self.particles_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/PSO-RF POP SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/PSO-RF ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T04:13:16.539849Z",
     "start_time": "2024-07-02T04:13:16.495928Z"
    }
   },
   "id": "c4c6224298ba6589",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### XGBoost",
   "id": "d8c75e15d55b3708"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T04:13:17.097155Z",
     "start_time": "2024-07-02T04:13:17.052996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PartXGB:\n",
    "    def __init_(self):\n",
    "        self.fitness = None\n",
    "        self.seed = None\n",
    "        self.estimators = 0\n",
    "        self.max_depth = 0\n",
    "        self.booster = None\n",
    "        self.reg_lambda = 0\n",
    "        self.reg_alpha = 0\n",
    "\n",
    "\n",
    "class PSOXGB:\n",
    "    def __init__(self, dataset, n_particles, n_iters, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iters = n_iters\n",
    "        self.particles = []\n",
    "        self.iters = []\n",
    "        self.BOOSTERS = [\"gbtree\", \"gblinear\", \"dart\"]\n",
    "        self.run()\n",
    "\n",
    "    def run(self):\n",
    "        lower_bound = [1, 1, 0, 0, 0]\n",
    "        uppper_bound = [300, 300, 2, 100, 100]\n",
    "        bounds = (lower_bound, uppper_bound)\n",
    "\n",
    "        options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
    "        optimizer = GlobalBestPSO(n_particles=self.n_particles,\n",
    "                                  dimensions=5,\n",
    "                                  options=options,\n",
    "                                  bounds=bounds)\n",
    "\n",
    "        optimizer.optimize(self.get_fitness, iters=self.n_iters)\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "\n",
    "    def get_fitness(self, parts):\n",
    "        parts = np.round(parts)\n",
    "        fit_lst = dask.compute([dask.delayed(self.objective_function)(parts[j]) for j in range(self.n_particles)])[0]\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "        self.iters.append(self.particles[0])\n",
    "        \n",
    "        self.save_parts_csv()\n",
    "        self.save_iters_csv()\n",
    "\n",
    "        gc.collect()\n",
    "        return fit_lst\n",
    "\n",
    "    def objective_function(self, particle_arr):\n",
    "        new_seed = self.seed * (2 + len(self.particles) / self.n_particles)\n",
    "        reset_seed(new_seed)\n",
    "        particle = PartXGB()\n",
    "        particle.estimators = int(particle_arr[0])\n",
    "        particle.max_depth = int(particle_arr[1])\n",
    "        particle.booster = self.BOOSTERS[int(particle_arr[2])]\n",
    "        particle.reg_lambda = float(particle_arr[3])\n",
    "        particle.reg_alpha = float(particle_arr[4])\n",
    "        particle.seed = new_seed\n",
    "\n",
    "        search = list(filter(lambda par:\n",
    "                             par.estimators == particle.estimators and\n",
    "                             par.max_depth == particle.max_depth and\n",
    "                             par.booster == particle.booster and\n",
    "                             par.reg_lambda == particle.reg_lambda and\n",
    "                             par.reg_alpha == particle.reg_alpha, self.particles))\n",
    "\n",
    "        if search:\n",
    "            self.particles.append(search[0])\n",
    "            return search[0].fitness\n",
    "\n",
    "        x = self.dataset.drop(\"consumo\", axis=1)\n",
    "        y = self.dataset[\"consumo\"]\n",
    "\n",
    "        updater = \"coord_descent\" if particle.booster == \"gblinear\" else None\n",
    "        model = XGBRegressor(device=\"cuda\", random_state=self.seed,\n",
    "                             n_estimators=particle.estimators,\n",
    "                             max_depth=particle.max_depth,\n",
    "                             booster=particle.booster,\n",
    "                             reg_lambda=particle.reg_lambda,\n",
    "                             reg_alpha=particle.reg_alpha,\n",
    "                             updater=updater)\n",
    "\n",
    "        csv = []\n",
    "        for i_train, i_test in TimeSeriesSplit(n_splits=6, test_size=1).split(x, y):\n",
    "            x_train, x_test = x.iloc[i_train].to_cupy().get(), x.iloc[i_test].to_cupy().get()\n",
    "            y_train, y_test = y.iloc[i_train].to_cupy().get(), y.iloc[i_test].to_cupy().get()\n",
    "\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            csv.append(int(mean_squared_error(y_test, model.predict(x_test).get())))\n",
    "            del x_train, x_test, y_train, y_test\n",
    "\n",
    "        self.particles.append(particle)\n",
    "\n",
    "        del new_seed, x, y, csv, i_train, i_test, model\n",
    "        gc.collect()\n",
    "        return particle.fitness\n",
    "\n",
    "    def particles_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for part in self.particles:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"N_estimators\": part.estimators,\n",
    "                \"Max_depth\": part.max_depth,\n",
    "                \"Booster\": part.booster,\n",
    "                \"Lambda\": part.reg_lambda,\n",
    "                \"Alpha\": part.reg_alpha,\n",
    "                \"Seed\": part.seed,\n",
    "                \"Fitness\": part.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for part in self.iters:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"N_estimators\": part.estimators,\n",
    "                \"Max_depth\": part.max_depth,\n",
    "                \"Booster\": part.booster,\n",
    "                \"Lambda\": part.reg_lambda,\n",
    "                \"Alpha\": part.reg_alpha,\n",
    "                \"Seed\": part.seed,\n",
    "                \"Fitness\": part.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def save_parts_csv(self):\n",
    "        pd_df = self.particles_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/PSO-XGB POP SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/PSO-XGB ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n"
   ],
   "id": "e77384b77f35ffe4",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SVR",
   "id": "a6da12a9f5e148c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T04:13:17.301436Z",
     "start_time": "2024-07-02T04:13:17.241238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PartSVR:\n",
    "    def __init_(self):\n",
    "        self.fitness = None\n",
    "        self.seed = None\n",
    "        self.c = 0\n",
    "        self.epsilon = 0\n",
    "        self.degree = 0\n",
    "        self.kernel = None\n",
    "        self.gamma = None\n",
    "\n",
    "\n",
    "class PSOSVR:\n",
    "    def __init__(self, dataset, n_particles, n_iters, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iters = n_iters\n",
    "        self.particles = []\n",
    "        self.iters = []\n",
    "        self.KERNELS = [\"poly\", \"rbf\", \"sigmoid\"]\n",
    "        self.run()\n",
    "\n",
    "    def run(self):\n",
    "        lower_bound = [0.001, 0.001, 0]\n",
    "        uppper_bound = [300, 300, 2]\n",
    "        bounds = (lower_bound, uppper_bound)\n",
    "\n",
    "        options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
    "        optimizer = GlobalBestPSO(n_particles=self.n_particles,\n",
    "                                  dimensions=3,\n",
    "                                  options=options,\n",
    "                                  bounds=bounds)\n",
    "\n",
    "        optimizer.optimize(self.get_fitness, iters=self.n_iters)\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "\n",
    "    def get_fitness(self, parts):\n",
    "        parts = np.round(parts)\n",
    "        fit_lst = dask.compute([dask.delayed(self.objective_function)(parts[j]) for j in range(self.n_particles)])[0]\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "        self.iters.append(self.particles[0])\n",
    "\n",
    "        self.save_parts_csv()\n",
    "        self.save_iters_csv()\n",
    "\n",
    "        gc.collect()\n",
    "        return fit_lst\n",
    "\n",
    "    def objective_function(self, particle_arr):\n",
    "        new_seed = self.seed * (2 + len(self.particles) / self.n_particles)\n",
    "        reset_seed(new_seed)\n",
    "        particle = PartSVR()\n",
    "        particle.c = float(particle_arr[0])\n",
    "        particle.epsilon = float(particle_arr[1])\n",
    "        particle.kernel = self.KERNELS[int(particle_arr[2])]\n",
    "        particle.seed = new_seed\n",
    "\n",
    "        search = list(filter(lambda par:\n",
    "                             par.c == particle.c and\n",
    "                             par.epsilon == particle.epsilon and\n",
    "                             par.degree == particle.degree and\n",
    "                             par.kernel == particle.kernel and\n",
    "                             par.gamma == particle.gamma, self.particles))\n",
    "\n",
    "        if search:\n",
    "            self.particles.append(search[0])\n",
    "            return search[0].fitness\n",
    "\n",
    "        x = self.dataset.drop(\"consumo\", axis=1)\n",
    "        y = self.dataset[\"consumo\"]\n",
    "\n",
    "        model = svr.SVR(C=particle.c,\n",
    "                        epsilon=particle.epsilon,\n",
    "                        kernel=particle.kernel)\n",
    "\n",
    "        csv = []\n",
    "        for i_train, i_test in TimeSeriesSplit(n_splits=6, test_size=1).split(x, y):\n",
    "            x_train, x_test = x.iloc[i_train].to_cupy().get(), x.iloc[i_test].to_cupy().get()\n",
    "            y_train, y_test = y.iloc[i_train].to_cupy().get(), y.iloc[i_test].to_cupy().get()\n",
    "\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            csv.append(int(mean_squared_error(y_test, model.predict(x_test).get())))\n",
    "            del x_train, x_test, y_train, y_test\n",
    "\n",
    "        self.particles.append(particle)\n",
    "\n",
    "        del new_seed, x, y, csv, i_train, i_test, model\n",
    "        gc.collect()\n",
    "        return particle.fitness\n",
    "\n",
    "    def particles_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for part in self.particles:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"C\": part.c,\n",
    "                \"Epsilon\": part.epsilon,\n",
    "                \"Kernel\": part.kernel,\n",
    "                \"Seed\": part.seed,\n",
    "                \"Fitness\": part.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for part in self.iters:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"C\": part.c,\n",
    "                \"Epsilon\": part.epsilon,\n",
    "                \"Kernel\": part.kernel,\n",
    "                \"Seed\": part.seed,\n",
    "                \"Fitness\": part.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def save_parts_csv(self):\n",
    "        pd_df = self.particles_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/PSO-SVR POP SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/PSO-SVR ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df"
   ],
   "id": "1ebffde56bc13e95",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LSTM",
   "id": "39d5fa0d022d9423"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T04:13:17.609503Z",
     "start_time": "2024-07-02T04:13:17.550210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PartLSTM:\n",
    "    def __init_(self):\n",
    "        self.fitness = None\n",
    "        self.seed = None\n",
    "        self.lstm_units = 0\n",
    "        self.epochs = 0\n",
    "        self.batch_size = 0\n",
    "        self.lstm_activation = None\n",
    "        self.bias = None\n",
    "\n",
    "\n",
    "class PSOLSTM:\n",
    "    def __init__(self, dataset, n_particles, n_iters, seed=SEED):\n",
    "        reset_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iters = n_iters\n",
    "        self.particles = []\n",
    "        self.iters = []\n",
    "        self.ACTIVATIONS = [\"linear\", \"mish\", \"sigmoid\", \"softmax\", \"softplus\", \"softsign\", \"tanh\", None]\n",
    "        self.BIAS = [False, True]\n",
    "        self.run()\n",
    "\n",
    "    def run(self):\n",
    "        lower_bound = [1, 1, 1, 0, 0]\n",
    "        uppper_bound = [300, 100, 300, 7, 1]\n",
    "        bounds = (lower_bound, uppper_bound)\n",
    "\n",
    "        options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
    "        optimizer = GlobalBestPSO(n_particles=self.n_particles,\n",
    "                                  dimensions=5,\n",
    "                                  options=options,\n",
    "                                  bounds=bounds)\n",
    "\n",
    "        optimizer.optimize(self.get_fitness, iters=self.n_iters)\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "\n",
    "    def get_fitness(self, parts):\n",
    "        parts = np.round(parts)\n",
    "        fit_lst = dask.compute([dask.delayed(self.objective_function)(parts[j]) for j in range(self.n_particles)])[0]\n",
    "        self.particles = sorted(self.particles, key=lambda a: a.fitness)\n",
    "        self.iters.append(self.particles[0])\n",
    "\n",
    "        self.save_parts_csv()\n",
    "        self.save_iters_csv()\n",
    "\n",
    "        gc.collect()\n",
    "        return fit_lst\n",
    "\n",
    "    def objective_function(self, particle_arr):\n",
    "        new_seed = self.seed * (2 + len(self.particles) / self.n_particles)\n",
    "        reset_seed(new_seed)\n",
    "        particle = PartLSTM()\n",
    "        particle.lstm_units = int(particle_arr[0])\n",
    "        particle.epochs = int(particle_arr[1])\n",
    "        particle.batch_size = int(particle_arr[2])\n",
    "        particle.lstm_activation = self.ACTIVATIONS[int(particle_arr[3])]\n",
    "        particle.bias = self.BIAS[int(particle_arr[4])]\n",
    "        particle.seed = new_seed\n",
    "\n",
    "        search = list(filter(lambda par:\n",
    "                             par.lstm_units == particle.lstm_units and\n",
    "                             par.epochs == particle.epochs and\n",
    "                             par.batch_size == particle.batch_size and\n",
    "                             par.lstm_activation == particle.lstm_activation and\n",
    "                             par.bias == particle.bias, self.particles))\n",
    "\n",
    "        if search:\n",
    "            self.particles.append(search[0])\n",
    "            return search[0].fitness\n",
    "\n",
    "        x = self.dataset.drop(\"consumo\", axis=1)\n",
    "        y = self.dataset[\"consumo\"]\n",
    "\n",
    "        model = Sequential([\n",
    "            Input((x.shape[1], 1)),\n",
    "            LSTM(particle.lstm_units,\n",
    "                 activation=particle.lstm_activation,\n",
    "                 use_bias=particle.bias,\n",
    "                 seed=self.seed),\n",
    "            Dense(1),\n",
    "        ])\n",
    "        model.compile(loss='mse')\n",
    "\n",
    "        csv = []\n",
    "        for i_train, i_test in TimeSeriesSplit(n_splits=6, test_size=1).split(x, y):\n",
    "            x_train, x_test = x.iloc[i_train].to_cupy().get(), x.iloc[i_test].to_cupy().get()\n",
    "            y_train, y_test = y.iloc[i_train].to_cupy().get(), y.iloc[i_test].to_cupy().get()\n",
    "\n",
    "            model.fit(x_train, y_train, shuffle=False, verbose=False, epochs=particle.epochs,\n",
    "                      batch_size=particle.batch_size)\n",
    "\n",
    "            csv.append(int(mean_squared_error(y_test, model.predict(x_test)[0])))\n",
    "            del x_train, x_test, y_train, y_test\n",
    "\n",
    "        self.particles.append(particle)\n",
    "\n",
    "        del new_seed, x, y, csv, i_train, i_test, model\n",
    "        gc.collect()\n",
    "        return particle.fitness\n",
    "\n",
    "    def particles_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for part in self.particles:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"Units\": part.lstm_units,\n",
    "                \"Epochs\": part.epochs,\n",
    "                \"Batch Size\": part.batch_size,\n",
    "                \"Activation\": part.lstm_activation,\n",
    "                \"Bias\": part.bias,\n",
    "                \"Seed\": part.seed,\n",
    "                \"Fitness\": part.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def iters_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        for part in self.iters:\n",
    "            df = pd.concat([df, pd.DataFrame({\n",
    "                \"Units\": part.lstm_units,\n",
    "                \"Epochs\": part.epochs,\n",
    "                \"Batch Size\": part.batch_size,\n",
    "                \"Activation\": part.lstm_activation,\n",
    "                \"Bias\": part.bias,\n",
    "                \"Seed\": part.seed,\n",
    "                \"Fitness\": part.fitness\n",
    "            })])\n",
    "        return df\n",
    "\n",
    "    def save_parts_csv(self):\n",
    "        pd_df = self.particles_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/PSO-LSTM POP SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df\n",
    "\n",
    "    def save_iters_csv(self):\n",
    "        pd_df = self.iters_dataframe().iters_dataframe().to_pandas()\n",
    "        pd_df.to_csv(f\"results/PSO-LSTM ITERS SEED {self.seed}.csv\", sep=\";\", decimal=\",\", index=True)\n",
    "        del pd_df"
   ],
   "id": "6c648af903ffab10",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Aplicação dos Otimizadores\n",
    "## Random Forest\n",
    "### Eletricidade\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fff4de69f44d41d"
  },
  {
   "cell_type": "code",
   "source": [
    "seeds = [10, 20, 30]\n",
    "\n",
    "\n",
    "def optimize_garf_electr(seed):\n",
    "    garf = GARF(df_electricity_selected, 50, 800, 0.5, seed)\n",
    "    garf.iters_dataframe().to_pandas().to_csv(f\"results/GA-RF ELECTR ITERs SEED {seed}.csv\", sep=\";\", decimal=\",\",\n",
    "                                              index=True)\n",
    "\n",
    "\n",
    "def optimize_psorf_electr(seed):\n",
    "    psorf = PSORF(df_electricity_selected, 50, 800, seed)\n",
    "    psorf.iters_dataframe().to_pandas().to_csv(f\"results/PSO-RF ELECTR ITERs SEED {seed}.csv\", sep=\";\", decimal=\",\",\n",
    "                                               index=True)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# futures = [optimize_garf_electr(seed) for seed in seeds]\n",
    "# dask.compute(futures)\n",
    "# futures = [optimize_psorf_electr(seed) for seed in seeds]\n",
    "# dask.compute(futures)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T04:13:17.891965Z",
     "start_time": "2024-07-02T04:13:17.869402Z"
    }
   },
   "id": "236caf6730ed692a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost\n",
    "### Eletricidade"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e74855edc797a8e6"
  },
  {
   "cell_type": "code",
   "source": [
    "seeds = [10, 20, 30]\n",
    "\n",
    "\n",
    "def optimize_gaxgb_electr(seed):\n",
    "    gaxgb = GAXGB(df_electricity_selected, 30, 300, 0.5, seed)\n",
    "    gaxgb.iters_dataframe().to_pandas().to_csv(f\"results/GA-XGB ELECTR ITERs SEED {seed}.csv\", sep=\";\", decimal=\",\",\n",
    "                                               index=True)\n",
    "\n",
    "\n",
    "def optimize_psoxgb_electr(seed):\n",
    "    psoxgb = PSOXGB(df_electricity_selected, 30, 300, seed)\n",
    "    psoxgb.iters_dataframe().to_pandas().to_csv(f\"results/PSO-XGB ELECTR ITERs SEED {seed}.csv\", sep=\";\",\n",
    "                                                decimal=\",\",\n",
    "                                                index=True)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# futures = [optimize_gaxgb_electr(seed) for seed in seeds]\n",
    "# dask.compute(futures)\n",
    "# futures = [optimize_psoxgb_electr(seed) for seed in seeds]\n",
    "# dask.compute(futures)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T04:13:18.035610Z",
     "start_time": "2024-07-02T04:13:18.018664Z"
    }
   },
   "id": "fc270813a7c3a24c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVR\n",
    "### Eletricidade\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27bdb2e123c383c1"
  },
  {
   "cell_type": "code",
   "source": [
    "seeds = [10, 20, 30]\n",
    "\n",
    "\n",
    "def optimize_gasvr_electr(seed):\n",
    "    gasvr = GASVR(df_electricity_selected, 30, 300, 0.5, seed)\n",
    "    gasvr.iters_dataframe().to_pandas().to_csv(f\"results/GA-SVR ELECTR ITERs SEED {seed}.csv\", sep=\";\", decimal=\",\",\n",
    "                                               index=True)\n",
    "\n",
    "\n",
    "def optimize_psosvr_electr(seed):\n",
    "    psosvr = PSOSVR(df_electricity_selected, 30, 300, seed)\n",
    "    psosvr.iters_dataframe().to_pandas().to_csv(f\"results/PSO-SVR ELECTR ITERs SEED {seed}.csv\", sep=\";\",\n",
    "                                                decimal=\",\",\n",
    "                                                index=True)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# futures = [optimize_gasvr_electr(seed) for seed in seeds]\n",
    "# dask.compute(futures)\n",
    "# futures = [optimize_psosvr_electr(seed) for seed in seeds]\n",
    "# dask.compute(futures)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T04:13:19.523428Z",
     "start_time": "2024-07-02T04:13:19.492599Z"
    }
   },
   "id": "84d7a6c888949595",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM\n",
    "### Eletricidade"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e8ddf2c403efbc8"
  },
  {
   "cell_type": "code",
   "source": [
    "seeds = [10, 20, 30]\n",
    "\n",
    "\n",
    "def optimize_galstm_electr(seed):\n",
    "    galstm = GALSTM(df_electricity_selected, 30, 300, 0.5, seed)\n",
    "\n",
    "\n",
    "def optimize_psolstm_electr(seed):\n",
    "    psolstm = PSOLSTM(df_electricity_selected, 30, 300, seed)\n",
    "    psolstm.iters_dataframe().to_pandas().to_csv(f\"results/PSO-LSTM ELECTR ITERs SEED {seed}.csv\", sep=\";\",\n",
    "                                                 decimal=\",\",\n",
    "                                                 index=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for seed in seeds:\n",
    "        optimize_galstm_electr(seed)\n",
    "        # futures = [optimize_galstm_electr(seed) for seed in seeds]\n",
    "    # dask.compute(futures)\n",
    "    # futures = [optimize_psolstm_electr(seed) for seed in seeds]\n",
    "    # dask.compute(futures)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-02T04:13:20.700547Z"
    }
   },
   "id": "129ca77e5926db8a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduardoalba0/.conda/envs/rapids-24.06/lib/python3.11/site-packages/dask/base.py:1462: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n",
      "2024-07-02 01:13:20.862569: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ind:14\n",
      "Units: 220Epochs: 86Batch Size: 51Activation: tanhBias: True\n",
      "Ind:6\n",
      "Units: 33Epochs: 60Batch Size: 85Activation: tanhBias: False\n",
      "Ind:28\n",
      "Units: 114Epochs: 86Batch Size: 124Activation: softsignBias: True\n",
      "Ind:22\n",
      "Units: 104Epochs: 88Batch Size: 188Activation: softmaxBias: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 01:13:20.874544: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-02 01:13:20.874822: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-02 01:13:20.889877: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-02 01:13:20.890224: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-02 01:13:20.890498: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-02 01:13:20.891739: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-02 01:13:20.892101: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-02 01:13:20.892181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-02 01:13:20.892808: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-02 01:13:20.893144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2236 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-07-02 01:13:53.719350: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1719893633.721231  229092 service.cc:145] XLA service 0x7fb4cc02b1c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1719893633.721395  229092 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2024-07-02 01:13:54.210291: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1719893642.429522  229092 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 4s/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 466ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 3s/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 3s/step\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Análise dos SHAP values dos modelos Otimizados",
   "id": "dca642eab8629e06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c8452d87c1c92374",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Previsões\n",
    "## Eletricidade\n",
    "### 3 Passos à frente\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29405b274310a71b"
  },
  {
   "cell_type": "code",
   "source": [
    "# reset_seed()\n",
    "# x_electricity = df_electricity.drop(\"consumo\", axis=1)\n",
    "# y_electricity = df_electricity[\"consumo\"]\n",
    "# \n",
    "# xgb_electricity = XGBRegressor()\n",
    "# rf_electricity = CudaRandomForest(n_streams=1, n_bins=x_electricity.shape[0])\n",
    "# svr_electricity = svr.SVR()\n",
    "# lstm_electricity = Sequential([\n",
    "#     Input((x_electricity.shape[1], 1)),\n",
    "#     LSTM(30, activation='relu', seed=SEED),\n",
    "#     Dense(1),\n",
    "# ])\n",
    "# lstm_electricity.compile(loss='mse')\n",
    "# \n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_electricity, y_electricity, test_size=3, shuffle=False)\n",
    "# \n",
    "# cvs_electricity = pd.DataFrame()\n",
    "# for i_train, i_test in TimeSeriesSplit(n_splits=6, test_size=1).split(x_train, y_train):\n",
    "#     kx_train, kx_test = x_train.iloc[i_train].to_numpy(), x_train.iloc[i_test].to_numpy()\n",
    "#     ky_train, ky_test = y_train.iloc[i_train].to_numpy(), y_train.iloc[i_test].to_numpy()\n",
    "# \n",
    "#     xgb_electricity.fit(kx_train, ky_train)\n",
    "#     rf_electricity.fit(kx_train, ky_train)\n",
    "#     svr_electricity.fit(kx_train, ky_train)\n",
    "#     lstm_electricity.fit(kx_train, ky_train, shuffle=False, verbose=False, epochs=1)\n",
    "#     cvs_electricity = pd.concat([cvs_electricity, pd.DataFrame({\n",
    "#         \"XGB\": mean_absolute_percentage_error(xgb_electricity.predict(kx_test), ky_test),\n",
    "#         \"RF\": mean_absolute_percentage_error(rf_electricity.predict(kx_test), ky_test),\n",
    "#         \"SVR\": mean_absolute_percentage_error(svr_electricity.predict(kx_test), ky_test),\n",
    "#         \"LSTM\": mean_absolute_percentage_error(lstm_electricity.predict(kx_test), ky_test)\n",
    "#     })])\n",
    "# \n",
    "# pred_xgb_electricity = []\n",
    "# for i_test in range(len(x_test)):\n",
    "#     sx_test = x_test.iloc[[i_test]]\n",
    "# \n",
    "#     for climatic_column in df_climatic.drop([\"ano\", \"mes\"], axis=1).columns:\n",
    "#         sx_test.at[sx_test.index, climatic_column] = \\\n",
    "#             x_electricity.at[(sx_test.index - pd.DateOffset(years=1)), climatic_column].to_numpy()[0][0]\n",
    "#     for lag in range(i_test + 1):\n",
    "#         if 'consumo_LAG_' + \"{:02d}\".format(lag) in sx_test.columns:\n",
    "#             sx_test['consumo_LAG_' + \"{:02d}\".format(lag)] = pred_xgb_electricity[-lag]\n",
    "# \n",
    "#     pred_xgb_electricity.append(xgb_electricity.predict(sx_test.to_numpy())[0])\n",
    "# \n",
    "# pred_rf_electricity = []\n",
    "# for i_test in range(len(x_test)):\n",
    "#     sx_test = x_test.iloc[[i_test]]\n",
    "# \n",
    "#     for climatic_column in df_climatic.drop([\"ano\", \"mes\"], axis=1).columns:\n",
    "#         sx_test.at[sx_test.index, climatic_column] = \\\n",
    "#             x_electricity.at[(sx_test.index - pd.DateOffset(years=1)), climatic_column].to_numpy()[0][0]\n",
    "#     for lag in range(i_test + 1):\n",
    "#         if 'consumo_LAG_' + \"{:02d}\".format(lag) in sx_test.columns:\n",
    "#             sx_test['consumo_LAG_' + \"{:02d}\".format(lag)] = pred_rf_electricity[-lag]\n",
    "# \n",
    "#     pred_rf_electricity.append(rf_electricity.predict(sx_test.to_numpy())[0])\n",
    "# \n",
    "# pred_svr_electricity = []\n",
    "# for i_test in range(len(x_test)):\n",
    "#     sx_test = x_test.iloc[[i_test]]\n",
    "# \n",
    "#     for climatic_column in df_climatic.drop([\"ano\", \"mes\"], axis=1).columns:\n",
    "#         sx_test.at[sx_test.index, climatic_column] = \\\n",
    "#             x_electricity.at[(sx_test.index - pd.DateOffset(years=1)), climatic_column].to_numpy()[0][0]\n",
    "#     for lag in range(i_test + 1):\n",
    "#         if 'consumo_LAG_' + \"{:02d}\".format(lag) in sx_test.columns:\n",
    "#             sx_test['consumo_LAG_' + \"{:02d}\".format(lag)] = pred_svr_electricity[-lag]\n",
    "# \n",
    "#     pred_svr_electricity.append(svr_electricity.predict(sx_test.to_numpy())[0])\n",
    "# \n",
    "# pred_lstm_electricity = []\n",
    "# for i_test in range(len(x_test)):\n",
    "#     sx_test = x_test.iloc[[i_test]]\n",
    "# \n",
    "#     for climatic_column in df_climatic.drop([\"ano\", \"mes\"], axis=1).columns:\n",
    "#         sx_test.at[sx_test.index, climatic_column] = \\\n",
    "#             x_electricity.at[(sx_test.index - pd.DateOffset(years=1)), climatic_column].to_numpy()[0][0]\n",
    "#     for lag in range(i_test + 1):\n",
    "#         if 'consumo_LAG_' + \"{:02d}\".format(lag) in sx_test.columns:\n",
    "#             sx_test['consumo_LAG_' + \"{:02d}\".format(lag)] = pred_lstm_electricity[-lag]\n",
    "# \n",
    "#     pred_lstm_electricity.append(lstm_electricity.predict(sx_test.to_numpy())[0])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "affa8389121b3b03",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6 Passos à frente"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16178f65debc9ff4"
  },
  {
   "cell_type": "code",
   "source": [
    "# reset_seed()\n",
    "# x_electricity = df_electricity.drop(\"consumo\", axis=1)\n",
    "# y_electricity = df_electricity[\"consumo\"]\n",
    "# \n",
    "# xgb_electricity = XGBRegressor()\n",
    "# rf_electricity = CudaRandomForest(n_streams=1, n_bins=x_electricity.shape[0])\n",
    "# svr_electricity = svr.SVR()\n",
    "# lstm_electricity = Sequential([\n",
    "#     Input((x_electricity.shape[1], 1)),\n",
    "#     LSTM(30, activation='relu', seed=SEED),\n",
    "#     Dense(1),\n",
    "# ])\n",
    "# lstm_electricity.compile(loss='mse')\n",
    "# \n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_electricity, y_electricity, test_size=6, shuffle=False)\n",
    "# \n",
    "# cvs_electricity = pd.DataFrame()\n",
    "# for i_train, i_test in TimeSeriesSplit(n_splits=6, test_size=1).split(x_train, y_train):\n",
    "#     kx_train, kx_test = x_train.iloc[i_train].to_numpy(), x_train.iloc[i_test].to_numpy()\n",
    "#     ky_train, ky_test = y_train.iloc[i_train].to_numpy(), y_train.iloc[i_test].to_numpy()\n",
    "# \n",
    "#     xgb_electricity.fit(kx_train, ky_train)\n",
    "#     rf_electricity.fit(kx_train, ky_train)\n",
    "#     svr_electricity.fit(kx_train, ky_train)\n",
    "#     lstm_electricity.fit(kx_train, ky_train, shuffle=False, verbose=False, epochs=1)\n",
    "#     cvs_electricity = pd.concat([cvs_electricity, pd.DataFrame({\n",
    "#         \"XGB\": mean_absolute_percentage_error(xgb_electricity.predict(kx_test), ky_test),\n",
    "#         \"RF\": mean_absolute_percentage_error(rf_electricity.predict(kx_test), ky_test),\n",
    "#         \"SVR\": mean_absolute_percentage_error(svr_electricity.predict(kx_test), ky_test),\n",
    "#         \"LSTM\": mean_absolute_percentage_error(lstm_electricity.predict(kx_test), ky_test)\n",
    "#     })])\n",
    "# \n",
    "# pred_xgb_electricity = []\n",
    "# for i_test in range(len(x_test)):\n",
    "#     sx_test = x_test.iloc[[i_test]]\n",
    "# \n",
    "#     for climatic_column in df_climatic.drop([\"ano\", \"mes\"], axis=1).columns:\n",
    "#         sx_test.at[sx_test.index, climatic_column] = \\\n",
    "#             x_electricity.at[(sx_test.index - pd.DateOffset(years=1)), climatic_column].to_numpy()[0][0]\n",
    "#     for lag in range(i_test + 1):\n",
    "#         if 'consumo_LAG_' + \"{:02d}\".format(lag) in sx_test.columns:  \n",
    "#             sx_test['consumo_LAG_' + \"{:02d}\".format(lag)] = pred_xgb_electricity[-lag]\n",
    "# \n",
    "#     pred_xgb_electricity.append(xgb_electricity.predict(sx_test.to_numpy())[0])\n",
    "# \n",
    "# pred_rf_electricity = []\n",
    "# for i_test in range(len(x_test)):\n",
    "#     sx_test = x_test.iloc[[i_test]]\n",
    "# \n",
    "#     for climatic_column in df_climatic.drop([\"ano\", \"mes\"], axis=1).columns:\n",
    "#         sx_test.at[sx_test.index, climatic_column] = \\\n",
    "#             x_electricity.at[(sx_test.index - pd.DateOffset(years=1)), climatic_column].to_numpy()[0][0]\n",
    "#     for lag in range(i_test + 1):\n",
    "#         if 'consumo_LAG_' + \"{:02d}\".format(lag) in sx_test.columns:  \n",
    "#             sx_test['consumo_LAG_' + \"{:02d}\".format(lag)] = pred_rf_electricity[-lag]\n",
    "# \n",
    "#     pred_rf_electricity.append(rf_electricity.predict(sx_test.to_numpy())[0])\n",
    "# \n",
    "# pred_svr_electricity = []\n",
    "# for i_test in range(len(x_test)):\n",
    "#     sx_test = x_test.iloc[[i_test]]\n",
    "# \n",
    "#     for climatic_column in df_climatic.drop([\"ano\", \"mes\"], axis=1).columns:\n",
    "#         sx_test.at[sx_test.index, climatic_column] = \\\n",
    "#             x_electricity.at[(sx_test.index - pd.DateOffset(years=1)), climatic_column].to_numpy()[0][0]\n",
    "#     for lag in range(i_test + 1):\n",
    "#         if 'consumo_LAG_' + \"{:02d}\".format(lag) in sx_test.columns:  \n",
    "#             sx_test['consumo_LAG_' + \"{:02d}\".format(lag)] = pred_svr_electricity[-lag]\n",
    "# \n",
    "#     pred_svr_electricity.append(svr_electricity.predict(sx_test.to_numpy())[0])\n",
    "# \n",
    "# pred_lstm_electricity = []\n",
    "# for i_test in range(len(x_test)):\n",
    "#     sx_test = x_test.iloc[[i_test]]\n",
    "# \n",
    "#     for climatic_column in df_climatic.drop([\"ano\", \"mes\"], axis=1).columns:\n",
    "#         sx_test.at[sx_test.index, climatic_column] = \\\n",
    "#             x_electricity.at[(sx_test.index - pd.DateOffset(years=1)), climatic_column].to_numpy()[0][0]\n",
    "#     for lag in range(i_test + 1):\n",
    "#         if 'consumo_LAG_' + \"{:02d}\".format(lag) in sx_test.columns:  \n",
    "#             sx_test['consumo_LAG_' + \"{:02d}\".format(lag)] = pred_lstm_electricity[-lag]\n",
    "# \n",
    "#     pred_lstm_electricity.append(lstm_electricity.predict(sx_test.to_numpy())[0])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6659cf7ebd8217fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 12 Passos à frente",
   "metadata": {
    "collapsed": false
   },
   "id": "4f58661c14d4c9ed"
  },
  {
   "cell_type": "code",
   "source": [
    "# reset_seed()\n",
    "# x_electricity = df_electricity.drop(\"consumo\", axis=1)\n",
    "# y_electricity = df_electricity[\"consumo\"]\n",
    "# \n",
    "# xgb_electricity = XGBRegressor()\n",
    "# rf_electricity = CudaRandomForest(n_streams=1, n_bins=x_electricity.shape[0])\n",
    "# svr_electricity = svr.SVR()\n",
    "# lstm_electricity = Sequential([\n",
    "#     Input((x_electricity.shape[1], 1)),\n",
    "#     LSTM(30, activation='relu', seed=SEED),\n",
    "#     Dense(1),\n",
    "# ])\n",
    "# lstm_electricity.compile(loss='mse')\n",
    "# \n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_electricity, y_electricity, test_size=12, shuffle=False)\n",
    "# \n",
    "# cvs_electricity = pd.DataFrame()\n",
    "# for i_train, i_test in TimeSeriesSplit(n_splits=6, test_size=1).split(x_train, y_train):\n",
    "#     kx_train, kx_test = x_train.iloc[i_train].to_numpy(), x_train.iloc[i_test].to_numpy()\n",
    "#     ky_train, ky_test = y_train.iloc[i_train].to_numpy(), y_train.iloc[i_test].to_numpy()\n",
    "# \n",
    "#     xgb_electricity.fit(kx_train, ky_train)\n",
    "#     rf_electricity.fit(kx_train, ky_train)\n",
    "#     svr_electricity.fit(kx_train, ky_train)\n",
    "#     lstm_electricity.fit(kx_train, ky_train, shuffle=False, verbose=False, epochs=1)\n",
    "#     cvs_electricity = pd.concat([cvs_electricity, pd.DataFrame({\n",
    "#         \"XGB\": mean_absolute_percentage_error(xgb_electricity.predict(kx_test), ky_test),\n",
    "#         \"RF\": mean_absolute_percentage_error(rf_electricity.predict(kx_test), ky_test),\n",
    "#         \"SVR\": mean_absolute_percentage_error(svr_electricity.predict(kx_test), ky_test),\n",
    "#         \"LSTM\": mean_absolute_percentage_error(lstm_electricity.predict(kx_test), ky_test)\n",
    "#     })])\n",
    "# \n",
    "# pred_xgb_electricity = []\n",
    "# for i_test in range(len(x_test)):\n",
    "#     sx_test = x_test.iloc[[i_test]]\n",
    "# \n",
    "#     for climatic_column in df_climatic.drop([\"ano\", \"mes\"], axis=1).columns:\n",
    "#         sx_test.at[sx_test.index, climatic_column] = \\\n",
    "#             x_electricity.at[(sx_test.index - pd.DateOffset(years=1)), climatic_column].to_numpy()[0][0]\n",
    "#     for lag in range(i_test + 1):\n",
    "#         if 'consumo_LAG_' + \"{:02d}\".format(lag) in sx_test.columns:  \n",
    "#             sx_test['consumo_LAG_' + \"{:02d}\".format(lag)] = pred_xgb_electricity[-lag]\n",
    "# \n",
    "#     pred_xgb_electricity.append(xgb_electricity.predict(sx_test.to_numpy())[0])\n",
    "# \n",
    "# pred_rf_electricity = []\n",
    "# for i_test in range(len(x_test)):\n",
    "#     sx_test = x_test.iloc[[i_test]]\n",
    "# \n",
    "#     for climatic_column in df_climatic.drop([\"ano\", \"mes\"], axis=1).columns:\n",
    "#         sx_test.at[sx_test.index, climatic_column] = \\\n",
    "#             x_electricity.at[(sx_test.index - pd.DateOffset(years=1)), climatic_column].to_numpy()[0][0]\n",
    "#     for lag in range(i_test + 1):\n",
    "#         if 'consumo_LAG_' + \"{:02d}\".format(lag) in sx_test.columns:  \n",
    "#             sx_test['consumo_LAG_' + \"{:02d}\".format(lag)] = pred_rf_electricity[-lag]\n",
    "# \n",
    "#     pred_rf_electricity.append(rf_electricity.predict(sx_test.to_numpy())[0])\n",
    "# \n",
    "# pred_svr_electricity = []\n",
    "# for i_test in range(len(x_test)):\n",
    "#     sx_test = x_test.iloc[[i_test]]\n",
    "# \n",
    "#     for climatic_column in df_climatic.drop([\"ano\", \"mes\"], axis=1).columns:\n",
    "#         sx_test.at[sx_test.index, climatic_column] = \\\n",
    "#             x_electricity.at[(sx_test.index - pd.DateOffset(years=1)), climatic_column].to_numpy()[0][0]\n",
    "#     for lag in range(i_test + 1):\n",
    "#         if 'consumo_LAG_' + \"{:02d}\".format(lag) in sx_test.columns:  \n",
    "#             sx_test['consumo_LAG_' + \"{:02d}\".format(lag)] = pred_svr_electricity[-lag]\n",
    "# \n",
    "#     pred_svr_electricity.append(svr_electricity.predict(sx_test.to_numpy())[0])\n",
    "# \n",
    "# pred_lstm_electricity = []\n",
    "# for i_test in range(len(x_test)):\n",
    "#     sx_test = x_test.iloc[[i_test]]\n",
    "# \n",
    "#     for climatic_column in df_climatic.drop([\"ano\", \"mes\"], axis=1).columns:\n",
    "#         sx_test.at[sx_test.index, climatic_column] = \\\n",
    "#             x_electricity.at[(sx_test.index - pd.DateOffset(years=1)), climatic_column].to_numpy()[0][0]\n",
    "#     for lag in range(i_test + 1):\n",
    "#         if 'consumo_LAG_' + \"{:02d}\".format(lag) in sx_test.columns:  \n",
    "#             sx_test['consumo_LAG_' + \"{:02d}\".format(lag)] = pred_lstm_electricity[-lag]\n",
    "# \n",
    "#     pred_lstm_electricity.append(lstm_electricity.predict(sx_test.to_numpy())[0])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53da7b76e64a75a3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
